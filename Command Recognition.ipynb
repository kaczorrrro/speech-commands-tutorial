{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from tensorflow.python.platform import gfile\n",
    "from tensorflow.contrib.framework.python.ops import audio_ops as contrib_audio\n",
    "from tensorflow.python.ops import io_ops\n",
    "from matplotlib import pyplot as plt\n",
    "from itertools import count\n",
    "import os\n",
    "%matplotlib "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use this if summaries live longer than they should\n",
    "#tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAGS = {}\n",
    "#FLAGS['dataset_dir'] = r\"/mnt/6A0850980850655B/Datasets/commnads/audio\"\n",
    "#FLAGS['dataset_dir'] = r\"/tmp/speech_dataset\"\n",
    "FLAGS['dataset_dir'] = r\"dev_audio\"\n",
    "FLAGS['wanted_words'] = ['nine', 'seven']\n",
    "FLAGS['background_dir'] = \"_background_noise_\"\n",
    "FLAGS['clip_time_ms'] = 1000\n",
    "FLAGS['window_time_ms'] = 30\n",
    "FLAGS['window_stride_ms'] = 10\n",
    "FLAGS['batch_size'] = 2\n",
    "FLAGS['sampling_rate'] = 16000\n",
    "FLAGS['mel_num_bins'] = 40\n",
    "FLAGS['mel_f_min'] = 40\n",
    "FLAGS['mel_f_max'] = FLAGS['sampling_rate']/2\n",
    "FLAGS['lr'] = 1e-4\n",
    "FLAGS['valid_percent'] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_audio(fs, data):\n",
    "    from io import BytesIO\n",
    "    from scipy.io import wavfile\n",
    "    import pyaudio \n",
    "    import wave  \n",
    "    \n",
    "    if data.dtype == np.float32:\n",
    "        if np.max(np.abs(data)) > 1.0:\n",
    "            raise RuntimeError(\"Float shoud be in range[-1,1]\")\n",
    "        data = (data*(1<<15)).astype(np.int16)\n",
    "    elif data.dtype == np.int16:\n",
    "        pass\n",
    "    else:\n",
    "        raise RuntimeError(\"Data type unsupported\")\n",
    "        \n",
    "    with BytesIO() as buffer:\n",
    "        p = pyaudio.PyAudio()\n",
    "        wavfile.write(buffer, fs, data)\n",
    "        f = wave.open(buffer,\"rb\")  \n",
    "        #define stream chunk   \n",
    "        chunk = 1024  \n",
    "        \n",
    "        stream = p.open(format = p.get_format_from_width(f.getsampwidth()),  \n",
    "                        channels = f.getnchannels(),  \n",
    "                        rate = f.getframerate(),  \n",
    "                        output = True)  \n",
    "        \n",
    "        #read data  \n",
    "        audio_data = f.readframes(chunk)\n",
    "\n",
    "        #play stream  \n",
    "        while audio_data:  \n",
    "            stream.write(audio_data)  \n",
    "            audio_data = f.readframes(chunk)  \n",
    "\n",
    "        #stop stream  \n",
    "        stream.stop_stream()  \n",
    "        stream.close()  \n",
    "        p.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_settings(clip_time_ms, sampling_rate, window_time_ms, window_stride_time_ms, mel_f_min, mel_f_max, mel_num_bins):\n",
    "    clip_samples = int(clip_time_ms * sampling_rate / 1000)\n",
    "    window_samples = int(window_time_ms * sampling_rate / 1000)\n",
    "    window_stride_samples = int(window_stride_time_ms * sampling_rate / 1000)\n",
    "    length_in_samples = 1 + int((clip_samples - window_samples) / window_stride_samples)\n",
    "    fft_size = int(2**np.ceil(np.log2(window_samples)))\n",
    "    settings = {\n",
    "        'clip_samples':          clip_samples,\n",
    "        'window_samples':        window_samples,\n",
    "        'window_stride_samples':window_stride_samples,\n",
    "        'fingerprint_size':      length_in_samples*mel_num_bins,\n",
    "        'sampling_rate':         sampling_rate,\n",
    "        'mel_f_min':             mel_f_min,\n",
    "        'mel_f_max':             mel_f_max,\n",
    "        'mel_num_bins':          mel_num_bins,\n",
    "        'lenght_in_samples':     length_in_samples,\n",
    "        'fft_size':              fft_size\n",
    "    }\n",
    "    return settings\n",
    "\n",
    "def get_training_settings(batch_size, valid_percent):\n",
    "    settings = {\n",
    "        'batch_size':batch_size,\n",
    "        'valid_percent':valid_percent\n",
    "    }\n",
    "    return settings\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class AudioProcessor:\n",
    "    def __init__(self, dataset_dir, background_dir, wanted_words, model_settings, training_settings):\n",
    "        self.model_settings = model_settings\n",
    "        self.training_settings = training_settings\n",
    "        self.id_to_label =  wanted_words\n",
    "        self.label_to_id = {label:i for i, label in enumerate(wanted_words)}\n",
    "        train_combined, valid_combined = self.load_audio(wanted_words, dataset_dir, background_dir)\n",
    "        #TODO change dataset to accept combined list\n",
    "        all_paths, all_labels = zip(*train_combined)\n",
    "        self.dataset = self.prepare_processing_graph(all_paths, all_labels, len(wanted_words))\n",
    "        self.iter = self.dataset.make_initializable_iterator()\n",
    "        self.mel_matrix = tf.constant(self.prepare_to_mel_matrix())\n",
    "        self.signals, self.labels = self.iter.get_next()\n",
    "        \n",
    "    def prepare_to_mel_matrix(self):\n",
    "        mel_num_bins = self.model_settings['mel_num_bins']\n",
    "        mel_f_min = self.model_settings['mel_f_min']\n",
    "        mel_f_max = self.model_settings['mel_f_max']\n",
    "        num_spectrogram_bins = self.model_settings['fft_size']//2+1\n",
    "        fs = self.model_settings['sampling_rate']\n",
    "\n",
    "        linear_to_mel_weight_matrix = tf.contrib.signal.linear_to_mel_weight_matrix(\n",
    "          mel_num_bins, num_spectrogram_bins, fs, mel_f_min, mel_f_max)\n",
    "        return linear_to_mel_weight_matrix.eval()\n",
    "        \n",
    "    def prepare_processing_graph(self, all_paths, all_labels, num_labels):\n",
    "        clip_samples = self.model_settings['clip_samples']\n",
    "        \n",
    "        def process_single_file(path):\n",
    "            wav_loader = io_ops.read_file(path)\n",
    "            audio, fs = contrib_audio.decode_wav(wav_loader, desired_channels=1, desired_samples=clip_samples)\n",
    "            tf.Assert(tf.equal(fs, self.model_settings['sampling_rate']), [fs])\n",
    "            audio = tf.squeeze(audio)\n",
    "            return audio\n",
    "        #dataset_placeholder = tf.placeholder(tf.string, (None, 2))\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(\n",
    "                    tf.convert_to_tensor(all_paths))\n",
    "        dataset = dataset.map(process_single_file)\n",
    "        \n",
    "        labels_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "            tf.convert_to_tensor(all_labels))\n",
    "        \n",
    "        #add labels to dataset\n",
    "        dataset = tf.data.Dataset.zip((dataset, labels_dataset))\n",
    "        \n",
    "        dataset = dataset.batch(self.training_settings['batch_size'])\n",
    "        return dataset\n",
    "\n",
    "        \n",
    "    def load_audio(self, wanted_words, dataset_dir, background_dir):\n",
    "        \"\"\"\n",
    "            Finds all wave files in dataset directory and arranges them into word -> list of files\n",
    "        \"\"\"\n",
    "        wanted_words = set(wanted_words)\n",
    "        all_paths = []\n",
    "        all_labels = []\n",
    "        search_path = os.path.join(dataset_dir, '*', '*.wav')\n",
    "        for wav_path in gfile.Glob(search_path):\n",
    "            _, word = os.path.split(os.path.dirname(wav_path))\n",
    "            if word == background_dir:\n",
    "                continue\n",
    "            if word in wanted_words:\n",
    "                all_paths.append(wav_path)\n",
    "                all_labels.append(self.label_to_id[word])\n",
    "#         all_paths = ['/media/sebastian/Itanos/Kursy/Audio signal processing coursera/sounds/piano.wav',\n",
    "#                      '/media/sebastian/Itanos/Kursy/Audio signal processing coursera/sounds/sine-440.wav']\n",
    "#         all_paths = ['I:/Kursy/Audio signal processing coursera/sounds/piano.wav',\n",
    "#                      'I:/Kursy/Audio signal processing coursera/sounds/sine-440.wav']\n",
    "#         all_labels = [0, 1]\n",
    "\n",
    "        #shuffle both list in the same way\n",
    "        combined = list(zip(all_paths,all_labels))\n",
    "        random.shuffle(combined)\n",
    "        valid_size = int(len(combined)*self.training_settings['valid_percent']/100)\n",
    "        valid_combined = combined[0:valid_size]\n",
    "        train_combined = combined[valid_size:]\n",
    "        \n",
    "        return train_combined, valid_combined\n",
    "    \n",
    "    def get_data(self, want_raw_spect: bool = False):\n",
    "        \"\"\"\n",
    "            Applies random transforms, and returns audio as spectrograms\n",
    "        \"\"\"\n",
    "        signals, labels = self.signals, self.labels\n",
    "        frame_len = self.model_settings['window_samples']\n",
    "        frame_step = self.model_settings['window_stride_samples']\n",
    "        fft_size = self.model_settings['fft_size']\n",
    "        spect = tf.contrib.signal.stft(signals, frame_len, frame_step, fft_size)\n",
    "        spect_mag = tf.abs(spect)\n",
    "        mel_spect = tf.tensordot(spect_mag, self.mel_matrix, 1)\n",
    "        if want_raw_spect:\n",
    "            return mel_spect, labels, spect\n",
    "        else:\n",
    "            return mel_spect, labels\n",
    "\n",
    "model_settings = get_model_settings(FLAGS['clip_time_ms'], FLAGS['sampling_rate'], FLAGS['window_time_ms'],\n",
    "                                    FLAGS['window_stride_ms'], FLAGS['mel_f_min'], FLAGS['mel_f_max'], FLAGS['mel_num_bins'])\n",
    "training_settings = get_training_settings(FLAGS['batch_size'], FLAGS['valid_percent'])\n",
    "ap = AudioProcessor(FLAGS['dataset_dir'], FLAGS['background_dir'], FLAGS['wanted_words'], model_settings, training_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_audio_from_mel(mel_spect, model_settings, true_spect = None):\n",
    "    mel_inv = np.linalg.pinv(ap.mel_matrix.eval())\n",
    "    spect_mag_reconstructed = (mel_spect @ mel_inv)\n",
    "    \n",
    "    if true_spect is not None:\n",
    "        phase = tf.angle(tf.constant(true_spect))\n",
    "    else:\n",
    "        phase = tf.random_uniform(spect_mag_reconstructed.shape, 2 * np.pi)\n",
    "        \n",
    "    spect_reconstructed = tf.complex(spect_mag_reconstructed * tf.cos(phase), spect_mag_reconstructed * tf.sin(phase))\n",
    "    audio = tf.contrib.signal.inverse_stft(spect_reconstructed, model_settings['window_samples'], \n",
    "                                           model_settings['window_stride_samples'], model_settings['fft_size'])\n",
    "    return audio.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectras, labels, spectras_true = ap.get_data(True)\n",
    "#spectras, labels, spectras_true = sess.run((spectras, labels, spectras_true))\n",
    "#play_audio(FLAGS['sampling_rate'],reconstruct_audio_from_mel(spectras[0,:,:], model_settings, spectras_true[0,:,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_model(model_settings, fingerprint, num_labels):\n",
    "    if fingerprint.shape[1] != model_settings['lenght_in_samples'] or fingerprint.shape[2] != model_settings['mel_num_bins']:\n",
    "        raise RuntimeError(\"Unexpected input: \" + str(fingerprint.shape))\n",
    "    h = tf.expand_dims(fingerprint, -1) #Add dimension at the end as as channels\n",
    "    h = tf.layers.conv2d(h, 64, [20, 8], activation=tf.nn.relu) #[time span, freq_span]\n",
    "    h = tf.layers.max_pooling2d(h, [1,3], [1,3])\n",
    "    h = tf.layers.conv2d(h, 64, [10, 4], activation=tf.nn.relu)\n",
    "    h = tf.layers.flatten(h)\n",
    "    h = tf.layers.dense(h, 128, activation=tf.nn.relu)\n",
    "    h = tf.layers.dense(h, num_labels)\n",
    "    scores = h\n",
    "    return scores\n",
    "        \n",
    "scores = conv_model(model_settings, spectras, len(FLAGS['wanted_words']))\n",
    "xent = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=scores)\n",
    "loss = tf.reduce_mean(xent)\n",
    "loss_summary = tf.summary.scalar(\"loss\", loss)\n",
    "pred = tf.argmax(scores, 1, output_type=tf.int32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(pred,labels), tf.float32))\n",
    "accuracy_summary = tf.summary.scalar(\"accuracy\", accuracy)\n",
    "merged_summary = tf.summary.merge([loss_summary, accuracy_summary])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = tf.train.AdamOptimizer(FLAGS['lr'])\n",
    "step = optim.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = tf.train.get_or_create_global_step()\n",
    "increment_step = tf.assign(global_step, global_step+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_writer = tf.summary.FileWriter('train_log', sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3945672207257964\n",
      "0.8939393939393939\n"
     ]
    }
   ],
   "source": [
    "ap.iter.initializer.run()\n",
    "e_sum = 0\n",
    "mean_acc = 0\n",
    "for i in count():\n",
    "    try:\n",
    "        train_summary, pred_val, labels_val, loss_val, acc_val, _, _ = sess.run([merged_summary, pred, labels, loss, accuracy, step, increment_step])\n",
    "        e_sum+=loss_val\n",
    "        mean_acc +=acc_val\n",
    "        train_writer.add_summary(train_summary, global_step.eval())\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        break\n",
    "print(e_sum/i)\n",
    "print(mean_acc/i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant([1,2])\n",
    "b = tf.constant([1,3])\n",
    "c = tf.equal(a,b)\n",
    "print(c.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [\"Spears\", \"Adele\", \"NDubz\", \"Nicole\", \"Cristina\"]\n",
    "b = [1, 2, 3, 4, 5]\n",
    "\n",
    "combined = list(zip(a, b))\n",
    "random.shuffle(combined)\n",
    "print(combined)\n",
    "\n",
    "a[:], b[:] = zip(*combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ąa'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [ ['ą','as'], ['b', 'bas'] ]\n",
    "p = tf.placeholder(tf.string, [None, 2])\n",
    "pa = p+'a'\n",
    "pa = sess.run(pa, feed_dict={p:data})\n",
    "str(pa[0][0].decode(\"utf-8\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
