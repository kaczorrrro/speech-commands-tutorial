{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: TkAgg\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from tensorflow.python.platform import gfile\n",
    "from tensorflow.contrib.framework.python.ops import audio_ops as contrib_audio\n",
    "from tensorflow.python.ops import io_ops\n",
    "from matplotlib import pyplot as plt\n",
    "from itertools import count\n",
    "import os\n",
    "%matplotlib "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use this if summaries live longer than they should\n",
    "tf.reset_default_graph()\n",
    "try:\n",
    "    isess.close()\n",
    "except NameError:\n",
    "    pass\n",
    "isess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAGS = {}\n",
    "FLAGS['dataset_dir'] = r\"/mnt/6A0850980850655B/Datasets/commnads/audio\"\n",
    "FLAGS['wanted_words'] = ['zero', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine'] \n",
    "\n",
    "FLAGS['background_dir'] = \"_background_noise_\"\n",
    "FLAGS['clip_time_ms'] = 1000\n",
    "FLAGS['window_time_ms'] = 30\n",
    "FLAGS['window_stride_ms'] = 10\n",
    "\n",
    "FLAGS['sampling_rate'] = 16000\n",
    "FLAGS['mel_num_bins'] = 40\n",
    "FLAGS['mel_f_min'] = 40\n",
    "FLAGS['mel_f_max'] = FLAGS['sampling_rate']/2\n",
    "\n",
    "FLAGS['lr'] = 1e-4\n",
    "FLAGS['batch_size'] = 128\n",
    "#dataset is extended by this percent\n",
    "FLAGS['unknown_percent'] = 10\n",
    "FLAGS['silence_percent'] = 10\n",
    "\n",
    "FLAGS['valid_percent'] = 10\n",
    "FLAGS['train_log'] = 'train_log'\n",
    "FLAGS['valid_log'] = 'valid_log'\n",
    "FLAGS['checkpoint_dir'] = 'checkpoints/conv.ckpt'\n",
    "FLAGS['background_volume'] = 0.1\n",
    "FLAGS['target_rms'] = 0.1\n",
    "FLAGS['max_shift_ms'] = 100\n",
    "FLAGS['background_percent'] = 50\n",
    "\n",
    "\n",
    "#override settings\n",
    "flags_dev ={\n",
    "    'dataset_dir':  r\"dev_audio\",\n",
    "    'wanted_words': ['nine', 'seven'],\n",
    "    'batch_size':   2\n",
    "}\n",
    "\n",
    "for flag in flags_dev:\n",
    "    FLAGS[flag] = flags_dev[flag]\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyaudio \n",
    "import wave\n",
    "\n",
    "def play_audio(fs, data):\n",
    "    from io import BytesIO\n",
    "    from scipy.io import wavfile\n",
    "      \n",
    "    \n",
    "    if data.dtype == np.float32:\n",
    "        max_abs = np.max(np.abs(data))\n",
    "        if max_abs > 1.0:\n",
    "            print(\"Loudest point was {}, so data will be scaled...\".format(max_abs))\n",
    "            data = data/max_abs\n",
    "        data = (data*(1<<15)).astype(np.int16)\n",
    "    elif data.dtype == np.int16:\n",
    "        pass\n",
    "    else:\n",
    "        raise RuntimeError(\"Data type unsupported\")\n",
    "        \n",
    "    with BytesIO() as buffer:\n",
    "        p = pyaudio.PyAudio()\n",
    "        wavfile.write(buffer, fs, data)\n",
    "        f = wave.open(buffer,\"rb\")  \n",
    "        #define stream chunk   \n",
    "        chunk = 1024  \n",
    "        \n",
    "        stream = p.open(format = p.get_format_from_width(f.getsampwidth()),  \n",
    "                        channels = f.getnchannels(),  \n",
    "                        rate = f.getframerate(),  \n",
    "                        output = True)  \n",
    "        \n",
    "        #read data  \n",
    "        audio_data = f.readframes(chunk)\n",
    "\n",
    "        #play stream  \n",
    "        while audio_data:  \n",
    "            stream.write(audio_data)  \n",
    "            audio_data = f.readframes(chunk)  \n",
    "\n",
    "        #stop stream  \n",
    "        stream.stop_stream()  \n",
    "        stream.close()  \n",
    "        p.terminate()\n",
    "        \n",
    "def record_audio(seconds, rate, countdown=True):        \n",
    "    from time import sleep\n",
    "    FORMAT = pyaudio.paFloat32\n",
    "    CHANNELS = 1\n",
    "    CHUNK = 1024\n",
    "    \n",
    "    audio = pyaudio.PyAudio()\n",
    "    stream = audio.open(format=FORMAT, channels=CHANNELS,\n",
    "                    rate=rate, input=True,\n",
    "                    frames_per_buffer=CHUNK)\n",
    "    if countdown:\n",
    "        for i in range(3,0,-1):\n",
    "            print(i)\n",
    "            sleep(0.5)\n",
    "    print(\"recording...\")\n",
    "    frames = []\n",
    "    for i in range(0, int(rate / CHUNK * seconds)):\n",
    "        data = stream.read(CHUNK)\n",
    "        frames.append(data)\n",
    "    print(\"finished recording\")\n",
    "\n",
    "    # stop Recording\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    audio.terminate()\n",
    "\n",
    "    raw_bytes = b''.join(frames)\n",
    "    samples = np.frombuffer(raw_bytes, dtype=np.float32)\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_settings(clip_time_ms, sampling_rate, window_time_ms, window_stride_time_ms, mel_f_min, mel_f_max, mel_num_bins, \n",
    "                       background_volume, background_percent, max_shift_ms):\n",
    "    clip_samples = int(clip_time_ms * sampling_rate / 1000)\n",
    "    window_samples = int(window_time_ms * sampling_rate / 1000)\n",
    "    max_shift_samples = int(max_shift_ms * sampling_rate / 1000)\n",
    "    window_stride_samples = int(window_stride_time_ms * sampling_rate / 1000)\n",
    "    length_in_samples = 1 + int((clip_samples - window_samples) / window_stride_samples)\n",
    "    fft_size = int(2**np.ceil(np.log2(window_samples)))\n",
    "    settings = {\n",
    "        'clip_samples':          clip_samples,\n",
    "        'window_samples':        window_samples,\n",
    "        'window_stride_samples': window_stride_samples,\n",
    "        'fingerprint_size':      length_in_samples*mel_num_bins,\n",
    "        'sampling_rate':         sampling_rate,\n",
    "        'mel_f_min':             mel_f_min,\n",
    "        'mel_f_max':             mel_f_max,\n",
    "        'mel_num_bins':          mel_num_bins,\n",
    "        'lenght_in_samples':     length_in_samples,\n",
    "        'fft_size':              fft_size,\n",
    "        'background_volume':     background_volume,\n",
    "        'background_percent':    background_percent,\n",
    "        'max_shift_samples':     max_shift_samples\n",
    "        \n",
    "    }\n",
    "    return settings\n",
    "\n",
    "def get_training_settings(batch_size, valid_percent, unknown_percent, silence_percent):\n",
    "    settings = {\n",
    "        'batch_size':          batch_size,\n",
    "        'valid_percent':       valid_percent,\n",
    "        'unknown_percent':     unknown_percent,\n",
    "        'silence_percent':     silence_percent\n",
    "    }\n",
    "    return settings\n",
    "\n",
    "model_settings = get_model_settings(FLAGS['clip_time_ms'], FLAGS['sampling_rate'], FLAGS['window_time_ms'],\n",
    "                                    FLAGS['window_stride_ms'], FLAGS['mel_f_min'], FLAGS['mel_f_max'], FLAGS['mel_num_bins'],\n",
    "                                    FLAGS['background_volume'], FLAGS['background_percent'], FLAGS['max_shift_ms'])\n",
    "training_settings = get_training_settings(FLAGS['batch_size'], FLAGS['valid_percent'], FLAGS['unknown_percent'], \n",
    "                                          FLAGS['silence_percent'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing mel matrix\n",
      "Preparing mel matrix finished\n",
      "Indexing audio files\n",
      "Examples per word:\n",
      "{'nine': 36, 'seven': 36, '_silence_': 7, '_unknown_': 7}\n",
      "Indexing audio files finished\n",
      "Loading backgrounds\n",
      "Loading backgrounds finished\n"
     ]
    }
   ],
   "source": [
    "#TODO: clip->normalize mixed signal, random shift\n",
    "class AudioProcessor:\n",
    "    def __init__(self, dataset_dir, background_dir, wanted_words, model_settings, training_settings):\n",
    "        self._model_settings = model_settings\n",
    "        self._mel_matrix = tf.constant(self.prepare_to_mel_matrix())\n",
    "        self.unknown_label, self.silence_label = '_unknown_', '_silence_'\n",
    "        self.id_to_label =  wanted_words + [self.unknown_label, self.silence_label]\n",
    "        self.label_to_id = {label:i for i, label in enumerate(self.id_to_label)}\n",
    "        self.train_set, self.valid_set, background_paths = self.load_audio(wanted_words, dataset_dir, background_dir, training_settings)\n",
    "        self._next_background = self.process_backgrounds(background_paths)\n",
    "        self.x_src, self.y_src, self.iter = self.prepare_datasets(len(wanted_words), training_settings)\n",
    "        self._signals, self._labels = self.iter.get_next()\n",
    "        \n",
    "    def prepare_to_mel_matrix(self):\n",
    "        print(\"Preparing mel matrix\")\n",
    "        mel_num_bins = self._model_settings['mel_num_bins']\n",
    "        mel_f_min = self._model_settings['mel_f_min']\n",
    "        mel_f_max = self._model_settings['mel_f_max']\n",
    "        num_spectrogram_bins = self._model_settings['fft_size']//2+1\n",
    "        fs = self._model_settings['sampling_rate']\n",
    "\n",
    "        linear_to_mel_weight_matrix = tf.contrib.signal.linear_to_mel_weight_matrix(\n",
    "          mel_num_bins, num_spectrogram_bins, fs, mel_f_min, mel_f_max)\n",
    "        with tf.Session() as sess:\n",
    "            out = linear_to_mel_weight_matrix.eval(session=sess)\n",
    "        print(\"Preparing mel matrix finished\")\n",
    "        return out\n",
    "    \n",
    "    def load_audio(self, wanted_words, dataset_dir, background_dir, training_settings):\n",
    "        \"\"\"\n",
    "            Finds all wave files in dataset directory and arranges them into word -> list of files\n",
    "        \"\"\"\n",
    "        print(\"Indexing audio files\")\n",
    "        wanted_words = {word: 0 for word in wanted_words}\n",
    "        index = []\n",
    "        unknown_index = []\n",
    "        background_paths = []\n",
    "        search_path = os.path.join(dataset_dir, '*', '*.wav')\n",
    "        for wav_path in gfile.Glob(search_path):\n",
    "            _, word = os.path.split(os.path.dirname(wav_path))\n",
    "            if word == background_dir:\n",
    "                background_paths.append(wav_path)\n",
    "                continue\n",
    "            if word in wanted_words:\n",
    "                wanted_words[word] += 1\n",
    "                index.append((wav_path, self.label_to_id[word]))\n",
    "            else:\n",
    "                unknown_index.append((wav_path, self.label_to_id[self.unknown_label]))\n",
    "        \n",
    "        if not all(wanted_words.values()):\n",
    "            raise RuntimeError(\"Didn't find any audio for {}\".format([w for w in wanted_words if not wanted_words[w]]))\n",
    "        \n",
    "        #append some unknown words\n",
    "        random.shuffle(unknown_index)\n",
    "        unknown_elems = min( int(round(len(index) * training_settings['unknown_percent'] / 100)), len(unknown_index) )\n",
    "        index.extend(unknown_index[:unknown_elems])\n",
    "        \n",
    "        #append silence - volume will be set to 0, so any filename is ok\n",
    "        silence_elems = int(len(index) * training_settings['silence_percent'] / 100)\n",
    "        silence_element = (index[0][0], self.label_to_id[self.silence_label]) # (some .wav file, silence index)\n",
    "        for _ in range(silence_elems):\n",
    "            index.append(silence_element)\n",
    "        \n",
    "        random.shuffle(index)\n",
    "        valid_size = int(len(index)*training_settings['valid_percent']/100)\n",
    "        \n",
    "        #list of tuples -> tuple of lists\n",
    "        valid = list(zip(*index[0:valid_size]))\n",
    "        train = list(zip(*index[valid_size:] ))\n",
    "        \n",
    "    \n",
    "        print(\"Examples per word:\")\n",
    "        print( {**wanted_words, self.silence_label: silence_elems, self.unknown_label:unknown_elems} )\n",
    "        print(\"Indexing audio files finished\")\n",
    "        return train, valid, background_paths\n",
    "    \n",
    "    def load_wav(self, path, desired_samples=-1):\n",
    "        wav_loader = io_ops.read_file(path)\n",
    "        audio, fs = contrib_audio.decode_wav(wav_loader, desired_channels=1, desired_samples=desired_samples)\n",
    "        tf.Assert(tf.equal(fs, self._model_settings['sampling_rate']), [fs])\n",
    "        audio = tf.squeeze(audio)\n",
    "        return audio\n",
    "    \n",
    "    def process_backgrounds(self, background_paths):\n",
    "        print(\"Loading backgrounds\")\n",
    "        path_input = tf.placeholder(tf.string)\n",
    "        wav_out = self.load_wav(path_input)\n",
    "        rms_scale_factor = (tf.sqrt(tf.reduce_mean(wav_out**2)) / FLAGS['target_rms'])\n",
    "        wav_out = tf.clip_by_value(wav_out / rms_scale_factor, -1, 1)\n",
    "        with tf.Session() as sess:\n",
    "            wavs = [ sess.run(wav_out, feed_dict={path_input:path}) for path in background_paths ]\n",
    "            \n",
    "        #to limit RAM usage, instead of using dataset.from_tensor_slices (which creates huge copies of\n",
    "        #of wav_out, taking more than x4 memory than they should) we create generator that returns data\n",
    "        #wav_out, that is referenced in this generator, is never freed since tf doesn't delete datasets\n",
    "        #this causes about 35 MB leak per AudioProcessor creation\n",
    "        #UPDATE - this leak should no longer happen - TODO check this\n",
    "        def gen():\n",
    "            num_examples = len(wavs)\n",
    "            clip_samples = self._model_settings['clip_samples']\n",
    "            while True:\n",
    "                example_no = random.randint(0, num_examples-1)\n",
    "                wav_data = wavs[example_no]\n",
    "                random_offset = random.randint(0, wav_data.size-1-clip_samples)\n",
    "                yield wav_data[random_offset:random_offset+clip_samples]\n",
    "                \n",
    "        next_background = tf.data.Dataset.from_generator(gen, tf.float32, output_shapes=(self._model_settings['clip_samples'],))\\\n",
    "                          .make_one_shot_iterator().get_next()\n",
    "        print(\"Loading backgrounds finished\")\n",
    "        return next_background\n",
    "        \n",
    "    def prepare_datasets(self, num_labels, training_settings):\n",
    "        clip_samples = self._model_settings['clip_samples']\n",
    "        \n",
    "        x_src = tf.placeholder(tf.string, (None,))\n",
    "        y_src = tf.placeholder(tf.int32,  (None,))\n",
    "        x_data = tf.data.Dataset.from_tensor_slices(x_src).map(lambda path: self.load_wav(path, clip_samples))\n",
    "        y_data = tf.data.Dataset.from_tensor_slices(y_src)\n",
    "        dataset = tf.data.Dataset.zip((x_data, y_data)).batch(training_settings['batch_size'])\n",
    "            \n",
    "        return x_src, y_src, dataset.make_initializable_iterator()\n",
    "    \n",
    "    def signal_to_mel(self, signals, debug_stuff = False):\n",
    "        \"\"\"\n",
    "            signals: [num_signals, num_signal_samples]\n",
    "        \"\"\"\n",
    "        frame_len = self._model_settings['window_samples']\n",
    "        frame_step = self._model_settings['window_stride_samples']\n",
    "        fft_size = self._model_settings['fft_size']\n",
    "        \n",
    "        \n",
    "        spect = tf.contrib.signal.stft(signals, frame_len, frame_step, fft_size)\n",
    "        spect_mag = tf.abs(spect)\n",
    "        mel_spect = tf.tensordot(spect_mag, self._mel_matrix, 1)\n",
    "        \n",
    "        if debug_stuff:\n",
    "            return spect, mel_spect\n",
    "        else:\n",
    "            return mel_spect\n",
    "        \n",
    "\n",
    "    def get_data(self, debug_stuff = False):\n",
    "        \"\"\"\n",
    "            Applies random transforms, and returns audio as spectrograms\n",
    "        \"\"\"\n",
    "        signals, labels = self._signals, self._labels\n",
    "        background = self._next_background[0:signals.shape[1]]\n",
    "        \n",
    "        #random volume, at most 'background volume', but non-zero only for 'background_percent' of samples\n",
    "        #tf.shape(signals)[0]\n",
    "        background_volume = tf.random_uniform((FLAGS['batch_size'],), 0, self._model_settings['background_volume'])\\\n",
    "                            * tf.cast(tf.random_uniform((FLAGS['batch_size'],), 0, 100)<self._model_settings['background_percent'], tf.float32)\n",
    "            \n",
    "        #volume = 0 for silence, normalize to target RMS for rest\n",
    "        rms = tf.sqrt(tf.reduce_mean(signals*signals, axis=1))\n",
    "        foreground_volume = tf.cast(tf.not_equal(labels, self.label_to_id[self.silence_label]), tf.float32)[:, None]\n",
    "        foreground_volume = foreground_volume / rms * FLAGS['target_rms']\n",
    "        \n",
    "        signals_mixed = foreground_volume * signals + background_volume[:, None] @ background[None, :]\n",
    "        signals_mixed = tf.clip_by_value(signals_mixed, -1, 1)\n",
    "\n",
    "        if debug_stuff:\n",
    "            spect, mel_spect = self.signal_to_mel(signals_mixed, debug_stuff)\n",
    "            return signals, background, signals_mixed, spect, mel_spect, labels\n",
    "        else:\n",
    "            mel_spect = self.signal_to_mel(signals_mixed, debug_stuff)\n",
    "            return mel_spect, labels\n",
    "            \n",
    "\n",
    "ap = AudioProcessor(FLAGS['dataset_dir'], FLAGS['background_dir'], FLAGS['wanted_words'], model_settings, training_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_audio_from_mel(mel_spect, model_settings, true_spect = None):\n",
    "    mel_inv = np.linalg.pinv(ap._mel_matrix.eval())\n",
    "    spect_mag_reconstructed = (mel_spect @ mel_inv)\n",
    "    \n",
    "    if true_spect is not None:\n",
    "        phase = tf.angle(tf.constant(true_spect))\n",
    "    else:\n",
    "        phase = tf.random_uniform(spect_mag_reconstructed.shape, 2 * np.pi)\n",
    "        \n",
    "    spect_reconstructed = tf.complex(spect_mag_reconstructed * tf.cos(phase), spect_mag_reconstructed * tf.sin(phase))\n",
    "    audio = tf.contrib.signal.inverse_stft(spect_reconstructed, model_settings['window_samples'], \n",
    "                                           model_settings['window_stride_samples'], model_settings['fft_size'])\n",
    "    return audio.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, ?) (?, 16000) (2,) (16000,)\n"
     ]
    }
   ],
   "source": [
    "signals, background, signals_mixed, spect, mel_spectras, labels = ap.get_data(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_example = False\n",
    "example_id = 0\n",
    "if play_example:\n",
    "#     ap.iter.initializer.run(feed_dict={ap.x_src: ap.train_set[0], ap.y_src: ap.train_set[1]})\n",
    "    vals = isess.run((signals, background, signals_mixed, spect, mel_spectras, labels))\n",
    "    s, b, sm, sp, ms, lab = [v[example_id] for v in vals]\n",
    "\n",
    "    print(ap.id_to_label[lab])\n",
    "    print(np.max(np.abs(sm)))\n",
    "    play_audio(FLAGS['sampling_rate'], reconstruct_audio_from_mel(ms, model_settings, sp) )#reconstructed with true phase\n",
    "    play_audio(FLAGS['sampling_rate'], reconstruct_audio_from_mel(ms, model_settings) )    #reconstructed without phase\n",
    "    play_audio(FLAGS['sampling_rate'], sm )                                                #mixed signals before stft    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_model(model_settings, fingerprint, num_labels, reuse=tf.AUTO_REUSE):\n",
    "    if fingerprint.shape[1] != model_settings['lenght_in_samples'] or fingerprint.shape[2] != model_settings['mel_num_bins']:\n",
    "        raise RuntimeError(\"Unexpected input: \" + str(fingerprint.shape))\n",
    "\n",
    "    h = tf.expand_dims(fingerprint, -1) #Add dimension at the end as as channels\n",
    "    h = tf.layers.conv2d(h, 64, [20, 8], activation=tf.nn.relu, name='conv1', reuse=reuse) #[time span, freq_span]\n",
    "    h = tf.layers.max_pooling2d(h, [1,3], [1,3])\n",
    "    h = tf.layers.conv2d(h, 64, [10, 4], activation=tf.nn.relu, name='conv2', reuse=reuse)\n",
    "    h = tf.layers.flatten(h)\n",
    "    h = tf.layers.dense(h, 128, activation=tf.nn.relu, name='dense1', reuse=reuse)\n",
    "    h = tf.layers.dense(h, num_labels, name='dense2', reuse=reuse)\n",
    "    scores = h\n",
    "    return scores\n",
    "        \n",
    "scores = conv_model(model_settings, mel_spectras, len(ap.id_to_label), reuse=False)\n",
    "xent = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=scores)\n",
    "loss = tf.reduce_mean(xent)\n",
    "loss_summary = tf.summary.scalar(\"loss\", loss)\n",
    "pred = tf.argmax(scores, 1, output_type=tf.int32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(pred,labels), tf.float32))\n",
    "accuracy_summary = tf.summary.scalar(\"accuracy\", accuracy)\n",
    "merged_summary = tf.summary.merge([loss_summary, accuracy_summary])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = tf.train.AdamOptimizer(FLAGS['lr'])\n",
    "step = optim.minimize(loss)\n",
    "global_step = tf.train.get_or_create_global_step()\n",
    "increment_step = tf.assign(global_step, global_step+1)\n",
    "\n",
    "saver = tf.train.Saver(tf.global_variables())\n",
    "train_writer = tf.summary.FileWriter('train_log', isess.graph)\n",
    "valid_writer = tf.summary.FileWriter('valid_log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saver.restore(isess, \"checkpoints/conv.ckpt-2223\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Incompatible shapes: [2] vs. [16000]\n\t [[Node: mul_4 = Mul[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](mul, strided_slice_1)]]\n\nCaused by op 'mul_4', defined at:\n  File \"C:\\Users\\sebas\\Miniconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\sebas\\Miniconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\sebas\\Miniconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\sebas\\Miniconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\sebas\\Miniconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 478, in start\n    self.io_loop.start()\n  File \"C:\\Users\\sebas\\Miniconda3\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Users\\sebas\\Miniconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 832, in start\n    self._run_callback(self._callbacks.popleft())\n  File \"C:\\Users\\sebas\\Miniconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 605, in _run_callback\n    ret = callback()\n  File \"C:\\Users\\sebas\\Miniconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\sebas\\Miniconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 263, in enter_eventloop\n    self.eventloop(self)\n  File \"C:\\Users\\sebas\\Miniconda3\\lib\\site-packages\\ipykernel\\eventloops.py\", line 228, in loop_tk\n    kernel.timer.start()\n  File \"C:\\Users\\sebas\\Miniconda3\\lib\\site-packages\\ipykernel\\eventloops.py\", line 225, in start\n    self.app.mainloop()\n  File \"C:\\Users\\sebas\\Miniconda3\\lib\\tkinter\\__init__.py\", line 1277, in mainloop\n    self.tk.mainloop(n)\n  File \"C:\\Users\\sebas\\Miniconda3\\lib\\tkinter\\__init__.py\", line 1699, in __call__\n    return self.func(*args)\n  File \"C:\\Users\\sebas\\Miniconda3\\lib\\tkinter\\__init__.py\", line 745, in callit\n    func(*args)\n  File \"C:\\Users\\sebas\\Miniconda3\\lib\\site-packages\\ipykernel\\eventloops.py\", line 220, in on_timer\n    self.func()\n  File \"C:\\Users\\sebas\\Miniconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 298, in do_one_iteration\n    stream.flush(zmq.POLLIN, 1)\n  File \"C:\\Users\\sebas\\Miniconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 352, in flush\n    self._handle_recv()\n  File \"C:\\Users\\sebas\\Miniconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\sebas\\Miniconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\sebas\\Miniconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\sebas\\Miniconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\sebas\\Miniconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\sebas\\Miniconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\sebas\\Miniconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\sebas\\Miniconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\sebas\\Miniconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\sebas\\Miniconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\sebas\\Miniconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-91-e38c0cf7812a>\", line 1, in <module>\n    signals, background, signals_mixed, spect, mel_spectras, labels = ap.get_data(True)\n  File \"<ipython-input-89-7e51a2f4fab5>\", line 156, in get_data\n    signals_mixed = foreground_volume * signals + background_volume * background\n  File \"C:\\Users\\sebas\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 934, in binary_op_wrapper\n    return func(x, y, name=name)\n  File \"C:\\Users\\sebas\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 1161, in _mul_dispatch\n    return gen_math_ops._mul(x, y, name=name)\n  File \"C:\\Users\\sebas\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\", line 3091, in _mul\n    \"Mul\", x=x, y=y, name=name)\n  File \"C:\\Users\\sebas\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\sebas\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3271, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\sebas\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1650, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): Incompatible shapes: [2] vs. [16000]\n\t [[Node: mul_4 = Mul[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](mul, strided_slice_1)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1360\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1361\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1362\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1339\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[1;32m-> 1340\u001b[1;33m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[0;32m   1341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[0;32m    515\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 516\u001b[1;33m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[0;32m    517\u001b[0m     \u001b[1;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Incompatible shapes: [2] vs. [16000]\n\t [[Node: mul_4 = Mul[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](mul, strided_slice_1)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-97-f7f673f58af5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m             \u001b[0mtrain_summary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0misess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmerged_summary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mincrement_step\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m             \u001b[0me_sum\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss_val\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[0mmean_acc\u001b[0m \u001b[1;33m+=\u001b[0m\u001b[0macc_val\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    903\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 905\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    906\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1135\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1137\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1138\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1353\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1355\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1356\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1357\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1372\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1373\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1374\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1375\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1376\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Incompatible shapes: [2] vs. [16000]\n\t [[Node: mul_4 = Mul[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](mul, strided_slice_1)]]\n\nCaused by op 'mul_4', defined at:\n  File \"C:\\Users\\sebas\\Miniconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\sebas\\Miniconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\sebas\\Miniconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\sebas\\Miniconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\sebas\\Miniconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 478, in start\n    self.io_loop.start()\n  File \"C:\\Users\\sebas\\Miniconda3\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Users\\sebas\\Miniconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 832, in start\n    self._run_callback(self._callbacks.popleft())\n  File \"C:\\Users\\sebas\\Miniconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 605, in _run_callback\n    ret = callback()\n  File \"C:\\Users\\sebas\\Miniconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\sebas\\Miniconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 263, in enter_eventloop\n    self.eventloop(self)\n  File \"C:\\Users\\sebas\\Miniconda3\\lib\\site-packages\\ipykernel\\eventloops.py\", line 228, in loop_tk\n    kernel.timer.start()\n  File \"C:\\Users\\sebas\\Miniconda3\\lib\\site-packages\\ipykernel\\eventloops.py\", line 225, in start\n    self.app.mainloop()\n  File \"C:\\Users\\sebas\\Miniconda3\\lib\\tkinter\\__init__.py\", line 1277, in mainloop\n    self.tk.mainloop(n)\n  File \"C:\\Users\\sebas\\Miniconda3\\lib\\tkinter\\__init__.py\", line 1699, in __call__\n    return self.func(*args)\n  File \"C:\\Users\\sebas\\Miniconda3\\lib\\tkinter\\__init__.py\", line 745, in callit\n    func(*args)\n  File \"C:\\Users\\sebas\\Miniconda3\\lib\\site-packages\\ipykernel\\eventloops.py\", line 220, in on_timer\n    self.func()\n  File \"C:\\Users\\sebas\\Miniconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 298, in do_one_iteration\n    stream.flush(zmq.POLLIN, 1)\n  File \"C:\\Users\\sebas\\Miniconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 352, in flush\n    self._handle_recv()\n  File \"C:\\Users\\sebas\\Miniconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\sebas\\Miniconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\sebas\\Miniconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\sebas\\Miniconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\sebas\\Miniconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\sebas\\Miniconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\sebas\\Miniconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\sebas\\Miniconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\sebas\\Miniconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\sebas\\Miniconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\sebas\\Miniconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-91-e38c0cf7812a>\", line 1, in <module>\n    signals, background, signals_mixed, spect, mel_spectras, labels = ap.get_data(True)\n  File \"<ipython-input-89-7e51a2f4fab5>\", line 156, in get_data\n    signals_mixed = foreground_volume * signals + background_volume * background\n  File \"C:\\Users\\sebas\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 934, in binary_op_wrapper\n    return func(x, y, name=name)\n  File \"C:\\Users\\sebas\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 1161, in _mul_dispatch\n    return gen_math_ops._mul(x, y, name=name)\n  File \"C:\\Users\\sebas\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\", line 3091, in _mul\n    \"Mul\", x=x, y=y, name=name)\n  File \"C:\\Users\\sebas\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\sebas\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3271, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\sebas\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1650, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): Incompatible shapes: [2] vs. [16000]\n\t [[Node: mul_4 = Mul[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](mul, strided_slice_1)]]\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "for epoch in range(epochs):\n",
    "    #train\n",
    "    ap.iter.initializer.run(feed_dict={ap.x_src: ap.train_set[0], ap.y_src: ap.train_set[1]})\n",
    "    e_sum = 0\n",
    "    mean_acc = 0\n",
    "    for i in count():\n",
    "        try:\n",
    "            train_summary, loss_val, acc_val, _, _ = isess.run([merged_summary, loss, accuracy, step, increment_step])\n",
    "            e_sum += loss_val\n",
    "            mean_acc +=acc_val\n",
    "            train_writer.add_summary(train_summary, global_step.eval())\n",
    "            print(\"{}/{}: loss {:.3f} acc {:.3f}\".format(i, \n",
    "                                                         len(ap.train_set[0])//FLAGS['batch_size'],\n",
    "                                                         loss_val, \n",
    "                                                         acc_val),\n",
    "                  end='\\r')\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break\n",
    "    print()\n",
    "    print(\"Epoch {}:{}loss {:.3f} acc {:.3f}\".format(epoch,\" \"*30, (e_sum/i), (mean_acc/i)))\n",
    "    print(\"Saving to {}\".format(FLAGS['checkpoint_dir']))\n",
    "    saver.save(isess, FLAGS['checkpoint_dir'], global_step=global_step.eval())\n",
    "    \n",
    "    #valid\n",
    "    ap.iter.initializer.run(feed_dict={ap.x_src: ap.valid_set[0], ap.y_src: ap.valid_set[1]})\n",
    "    e_sum = 0\n",
    "    mean_acc = 0\n",
    "    for i in count():\n",
    "        try:\n",
    "            valid_summary, loss_val, acc_val, = isess.run([merged_summary, loss, accuracy])\n",
    "            e_sum += loss_val\n",
    "            mean_acc += acc_val\n",
    "            valid_writer.add_summary(valid_summary, epoch)\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break\n",
    "    print(\"Validation {}:{}loss {:.3f} acc {:.3f}\".format(epoch,\" \"*50, (e_sum/i), (mean_acc/i)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_file_processing_graph(filename):\n",
    "    audio_tensor = ap.load_wav(filename, model_settings['clip_samples'])\n",
    "    return raw_data_processing_graph(audio_tensor)\n",
    "    \n",
    "def raw_data_processing_graph(audio_tensor):\n",
    "    #audio_tensor [samples, ] float32 tensor\n",
    "    audio_tensor = audio_tensor[None, :]\n",
    "    mel_spect = ap.signal_to_mel(audio_tensor)\n",
    "    nn_scores = conv_model(model_settings, mel_spect, len(ap.id_to_label))\n",
    "    pred_label_id = tf.argmax(nn_scores, 1, output_type=tf.int32)\n",
    "    return nn_scores, pred_label_id\n",
    "\n",
    "\n",
    "filename_input = tf.placeholder(tf.string, shape=[])\n",
    "audio_tensor_input = tf.placeholder(tf.float32, shape=[model_settings['clip_samples']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: seven, true seven\n",
      "Predicted: seven, true seven\n",
      "Predicted: nine, true nine\n",
      "Predicted: seven, true seven\n",
      "Predicted: nine, true nine\n",
      "Predicted: seven, true seven\n",
      "Predicted: nine, true nine\n",
      "Predicted: seven, true seven\n",
      "Predicted: nine, true nine\n",
      "Predicted: seven, true seven\n"
     ]
    }
   ],
   "source": [
    "#Test some audio from files - by default from training set\n",
    "nn_scores, pred_label_id = single_file_processing_graph(filename_input)\n",
    "for i in range(10):\n",
    "    path, true_label_id = ap.train_set[0][i], ap.train_set[1][i]\n",
    "    if true_label_id == ap.label_to_id[ap.silence_label]:\n",
    "        continue #here no tracks are mixed/no volume adjustment, so silence doesn't make sense\n",
    "    pred_label_id_val = pred_label_id.eval(feed_dict={filename_input: path})[0]\n",
    "    mark = \"\" if true_label_id == pred_label_id_val else \"!\"\n",
    "    \n",
    "    print(\"{}Predicted: {}, true {}\".format(mark, ap.id_to_label[pred_label_id_val], ap.id_to_label[true_label_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Record some audio and test\n",
    "nn_scores, pred_label = raw_data_processing_graph(audio_tensor_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "2\n",
      "1\n",
      "recording...\n",
      "finished recording\n",
      "I heard three\n"
     ]
    }
   ],
   "source": [
    "fs = FLAGS['sampling_rate']\n",
    "output = record_audio(FLAGS['clip_time_ms']/1000, fs)\n",
    "play_audio(fs, output)\n",
    "output_padded = np.pad(output, (0, model_settings['clip_samples'] - output.shape[0]), 'constant')\n",
    "label_id = pred_label.eval(feed_dict={audio_tensor_input:output_padded})[0]\n",
    "print(\"I heard {}\".format(ap.id_to_label[label_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recording...\n",
      "I heard six\n",
      "I heard zero\n",
      "I heard six\n",
      "I heard zero\n",
      "I heard one\n",
      "I heard _unknown_\n",
      "I heard seven\n",
      "I heard three\n",
      "I heard zero\n",
      "I heard _unknown_\n",
      "I heard two\n",
      "I heard _unknown_\n",
      "I heard six\n",
      "I heard zero\n",
      "I heard four\n",
      "I heard _unknown_\n",
      "I heard one\n",
      "I heard zero\n",
      "I heard nine\n",
      "I heard _unknown_\n",
      "I heard nine\n",
      "I heard _unknown_\n",
      "I heard five\n",
      "I heard _unknown_\n",
      "I heard eight\n",
      "I heard six\n",
      "I heard zero\n",
      "finished recording\n"
     ]
    }
   ],
   "source": [
    "from threading import Event, Thread\n",
    "from math import ceil\n",
    "\n",
    "\n",
    "event = Event()\n",
    "\n",
    "buffer = None\n",
    "frames = None\n",
    "samples_ready = 0\n",
    "def record_audio(seconds, rate, event):       \n",
    "    global buffer, frames, samples_ready\n",
    "    FORMAT = pyaudio.paFloat32\n",
    "    CHANNELS = 1\n",
    "    CHUNK = 1024\n",
    "    \n",
    "    frames = int(rate / CHUNK * seconds)\n",
    "    buffer = np.zeros(CHUNK*frames)\n",
    "    event.set()\n",
    "    audio = pyaudio.PyAudio()\n",
    "    stream = audio.open(format=FORMAT, channels=CHANNELS,\n",
    "                    rate=rate, input=True,\n",
    "                    frames_per_buffer=CHUNK)\n",
    "\n",
    "    print(\"recording...\")\n",
    "    for i in range(frames):\n",
    "        data = stream.read(CHUNK)\n",
    "        buffer[i*CHUNK: (i+1)*CHUNK] = np.frombuffer(data, dtype=np.float32)\n",
    "        samples_ready = (i+1)*CHUNK\n",
    "        event.set()\n",
    "    print(\"finished recording\")\n",
    "\n",
    "    # stop Recording\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    audio.terminate()\n",
    "\n",
    "recorder = Thread(target=record_audio, args=(10, FLAGS['sampling_rate'], event))\n",
    "recorder.start()\n",
    "\n",
    "event.wait()\n",
    "event.clear()\n",
    "\n",
    "run_every_ms = 100\n",
    "run_every_samples =  int(ceil(FLAGS['sampling_rate']*run_every_ms/1000))\n",
    "next_start_sample = 0\n",
    "next_end_sample = model_settings['clip_samples']\n",
    "last_id = -1\n",
    "for i in range(frames):\n",
    "    event.wait()\n",
    "    if samples_ready > next_end_sample:\n",
    "        label_id = pred_label.eval(feed_dict={audio_tensor_input:buffer[next_start_sample:next_end_sample]})[0]\n",
    "        if label_id != last_id and label_id != ap.label_to_id[ap.silence_label]:\n",
    "            print(\"I heard {}\".format(ap.id_to_label[label_id]))\n",
    "            last_id = label_id\n",
    "        next_start_sample += run_every_samples\n",
    "        next_end_sample   += run_every_samples\n",
    "    event.clear()\n",
    "    \n",
    "\n",
    "recorder.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [ ['ą','as'], ['b', 'bas'] ]\n",
    "p = tf.placeholder(tf.string, [None, 2])\n",
    "pa = p+'a'\n",
    "pa = sess.run(pa, feed_dict={p:data})\n",
    "str(pa[0][0].decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.random_uniform((1,), 0, 1)>0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True]\n",
      "[ True]\n",
      "[ True]\n",
      "[False]\n",
      "[False]\n",
      "[ True]\n",
      "[ True]\n",
      "[False]\n",
      "[ True]\n",
      "[False]\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(a.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
